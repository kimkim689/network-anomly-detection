{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcbKc6zHE13v"
      },
      "source": [
        "# RANDOM FOREST MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7l-wg8QDdkv"
      },
      "source": [
        "#STEP 1 : Load the training and test data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BzFkM3-iMK7E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset directory: c:\\Users\\kimki\\network_anomaly_detect\\system_main\\dataset\n",
            "Model directory: c:\\Users\\kimki\\network_anomaly_detect\\system_main\\model\\saved_model\n",
            "Training data shape: (175341, 45)\n",
            "Testing data shape: (82332, 45)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import os \n",
        "\n",
        "# Step 1: Load the training and test data\n",
        "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "project_root = current_dir\n",
        "while not os.path.exists(os.path.join(project_root, 'system_main')):\n",
        "    project_root = os.path.dirname(project_root)\n",
        "    if project_root == os.path.dirname(project_root):  # reached the root directory\n",
        "        raise FileNotFoundError(\"Could not find the 'system_main' directory.\")\n",
        "\n",
        "# Set up paths\n",
        "dataset_dir = os.path.join(project_root, 'system_main', 'dataset')\n",
        "model_dir = os.path.join(project_root, 'system_main', 'model', 'saved_model')\n",
        "\n",
        "# Ensure the directories exist\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Print the paths for verification\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Model directory: {model_dir}\")\n",
        "\n",
        "# Load the data\n",
        "train_data_path = os.path.join(dataset_dir, 'UNSW_NB15_training-set.csv')\n",
        "test_data_path = os.path.join(dataset_dir, 'UNSW_NB15_testing-set.csv')\n",
        "\n",
        "# Check if the files exist\n",
        "if not os.path.exists(train_data_path):\n",
        "    raise FileNotFoundError(f\"Training dataset not found at {train_data_path}\")\n",
        "if not os.path.exists(test_data_path):\n",
        "    raise FileNotFoundError(f\"Testing dataset not found at {test_data_path}\")\n",
        "\n",
        "# Load the data\n",
        "train_data_initial = pd.read_csv(train_data_path)\n",
        "test_data_initial = pd.read_csv(test_data_path)\n",
        "\n",
        "# Print the shape of the loaded data for verification\n",
        "print(f\"Training data shape: {train_data_initial.shape}\")\n",
        "print(f\"Testing data shape: {test_data_initial.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "theeN3hOMT2d"
      },
      "source": [
        "#STEP 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def engineer_features(df):\n",
        "    # Print available columns for debugging\n",
        "    print(\"Available columns:\", df.columns.tolist())\n",
        "\n",
        "    # Create a feature for the total number of bytes\n",
        "    if 'sbytes' in df.columns and 'dbytes' in df.columns:\n",
        "        df['total_bytes'] = df['sbytes'] + df['dbytes']\n",
        "    \n",
        "    # Create a feature for the ratio of source to destination bytes\n",
        "    if 'sbytes' in df.columns and 'dbytes' in df.columns:\n",
        "        df['byte_ratio'] = df['sbytes'] / (df['dbytes'] + 1)  # Adding 1 to avoid division by zero\n",
        "    \n",
        "    # Create a feature for the difference in TTL\n",
        "    if 'sttl' in df.columns and 'dttl' in df.columns:\n",
        "        df['ttl_diff'] = df['sttl'] - df['dttl']\n",
        "    \n",
        "    # Create a binary feature for high port numbers \n",
        "    if 'sport' in df.columns and 'dport' in df.columns:\n",
        "        df['high_port'] = ((df['sport'] > 1024) | (df['dport'] > 1024)).astype(int)\n",
        "    \n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "mzen9-2UNdEs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available columns: ['id', 'dur', 'proto', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label']\n",
            "Available columns: ['id', 'dur', 'proto', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat', 'label']\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Data-Preprocessing\n",
        "# Feature Engineering Function to make up the lost of attack_cat feature \n",
        "\n",
        "\n",
        "# Drop the 'service' column from both training and test datasets as it contains a lot of missing values\n",
        "train_data = train_data_initial.drop(columns=['service'])\n",
        "test_data = test_data_initial.drop(columns=['service'])\n",
        "# Apply feature engineering\n",
        "train_data = engineer_features(train_data)\n",
        "test_data = engineer_features(test_data)\n",
        "\n",
        "# Define the target column and categorical columns\n",
        "target_column = 'label'\n",
        "categorical_columns = ['proto', 'state','attack_cat']\n",
        "# Separate features and target from the training data\n",
        "X_train = train_data.drop(columns=[target_column])\n",
        "y_train = train_data[target_column]\n",
        "\n",
        "# Separate features and target from the test data\n",
        "X_test = test_data.drop(columns=[target_column])\n",
        "y_test = test_data[target_column]\n",
        "\n",
        "# Initialize label encoders for categorical columns\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(X_train[col].astype(str))  # Fit on the training data\n",
        "    X_train[col] = le.transform(X_train[col].astype(str))\n",
        "\n",
        "    # Handle unseen categories in the test set\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "    X_test[col] = X_test[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n",
        "\n",
        "    label_encoders[col] = le"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IjPpPfMESCG"
      },
      "source": [
        "# STEP 3 : Initialize and train the basic Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cCdG4EENnmU",
        "outputId": "8ce29ac1-a40c-4ebb-982e-6bcb35726fa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic Model Accuracy: 0.86\n",
            "Confusion Matrix:\n",
            "[[36996     4]\n",
            " [11697 33635]]\n",
            "Classification Report for Basic Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      1.00      0.86     37000\n",
            "           1       1.00      0.74      0.85     45332\n",
            "\n",
            "    accuracy                           0.86     82332\n",
            "   macro avg       0.88      0.87      0.86     82332\n",
            "weighted avg       0.89      0.86      0.86     82332\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Initialize and train the basic Random Forest model\n",
        "basic_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "basic_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data using the basic model\n",
        "y_pred = basic_model.predict(X_test)\n",
        "\n",
        "# Evaluate the basic model\n",
        "basic_accuracy = accuracy_score(y_test, y_pred)\n",
        "basic_report = classification_report(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f'Basic Model Accuracy: {basic_accuracy:.2f}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "print('Classification Report for Basic Model:')\n",
        "print(basic_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d--0qThEhSs"
      },
      "source": [
        "# STEP 4: Model Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cta9Lyn8cNae"
      },
      "source": [
        "When we further analysed the UNSW-NB15, we noticed that it is an imbalanced dataset. The SMOTE (Synthetic Minority Oversampling Technique) algorithm is incorporated to create synthetic samples for the minority class to balance out the class distribution of the dataset. Then we optimize the model again over the balanced training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lohXovYSb-4N"
      },
      "outputs": [],
      "source": [
        "#Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXjSQ49JcAWM",
        "outputId": "586777cf-55de-4c5e-ac23-4117db02949e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kimki\\network_anomaly_detect\\.venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized Model Accuracy: 0.97\n",
            "Confusion Matrix for Optimized Model:\n",
            "[[37000     0]\n",
            " [ 2551 42781]]\n",
            "Classification Report for Optimized Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97     37000\n",
            "           1       1.00      0.94      0.97     45332\n",
            "\n",
            "    accuracy                           0.97     82332\n",
            "   macro avg       0.97      0.97      0.97     82332\n",
            "weighted avg       0.97      0.97      0.97     82332\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Model Optimization\n",
        "#Feature selection using SelectKBest after the basic model\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=25)  # Selecting top 20 features\n",
        "X_train_selected = selector.fit_transform(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_feature_names = X_train_resampled.columns[selector.get_support()]\n",
        "\n",
        "# Apply the same selection to test data\n",
        "X_test_selected = X_test[selected_feature_names]\n",
        "\n",
        "# Train a new Random Forest model on the selected features\n",
        "optimized_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "optimized_model.fit(X_train_selected, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test data using the optimized model\n",
        "y_pred_optimized = optimized_model.predict(X_test_selected)\n",
        "\n",
        "# Evaluate the optimized model\n",
        "optimized_accuracy = accuracy_score(y_test, y_pred_optimized)\n",
        "optimized_report = classification_report(y_test, y_pred_optimized)\n",
        "optimized_conf_matrix = confusion_matrix(y_test, y_pred_optimized)\n",
        "\n",
        "print(f'Optimized Model Accuracy: {optimized_accuracy:.2f}')\n",
        "print('Confusion Matrix for Optimized Model:')\n",
        "print(optimized_conf_matrix)\n",
        "print('Classification Report for Optimized Model:')\n",
        "print(optimized_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: c:\\Users\\kimki\\network_anomaly_detect\\system_main\\model\\saved_model\\random_forest_model.joblib\n",
            "Preprocessor saved to: c:\\Users\\kimki\\network_anomaly_detect\\system_main\\model\\saved_model\\rf_preprocessor.joblib\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "# Save the model and preprocessor information\n",
        "model_path = os.path.join(model_dir, 'random_forest_model.joblib')\n",
        "joblib.dump(optimized_model, model_path)\n",
        "\n",
        "preprocessor = {\n",
        "    'label_encoders': label_encoders,\n",
        "    'selected_features': selected_feature_names,\n",
        "    'feature_selector': selector,\n",
        "    'smote': smote\n",
        "}\n",
        "preprocessor_path = os.path.join(model_dir, 'rf_preprocessor.joblib')\n",
        "joblib.dump(preprocessor, preprocessor_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Preprocessor saved to: {preprocessor_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
