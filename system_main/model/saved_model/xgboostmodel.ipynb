{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4PcNPZkwl8h"
      },
      "source": [
        "# Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "snavJ-gFtK9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset directory: c:\\Users\\kimki\\network_anomaly_detect\\system_main\\dataset\n",
            "Model directory: c:\\Users\\kimki\\network_anomaly_detect\\system_main\\model\\saved_model\n",
            "Training data shape: (175341, 45)\n",
            "Testing data shape: (82332, 45)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "import os \n",
        "import sys\n",
        "# Step 1: Load the training and test data\n",
        "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
        "project_root = current_dir\n",
        "while not os.path.exists(os.path.join(project_root, 'system_main')):\n",
        "    project_root = os.path.dirname(project_root)\n",
        "    if project_root == os.path.dirname(project_root):  # reached the root directory\n",
        "        raise FileNotFoundError(\"Could not find the 'system_main' directory.\")\n",
        "\n",
        "# Set up paths\n",
        "dataset_dir = os.path.join(project_root, 'system_main', 'dataset')\n",
        "model_dir = os.path.join(project_root, 'system_main', 'model', 'saved_model')\n",
        "\n",
        "# Ensure the directories exist\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Print the paths for verification\n",
        "print(f\"Dataset directory: {dataset_dir}\")\n",
        "print(f\"Model directory: {model_dir}\")\n",
        "\n",
        "# Load the data\n",
        "train_data_path = os.path.join(dataset_dir, 'UNSW_NB15_training-set.csv')\n",
        "test_data_path = os.path.join(dataset_dir, 'UNSW_NB15_testing-set.csv')\n",
        "\n",
        "# Check if the files exist\n",
        "if not os.path.exists(train_data_path):\n",
        "    raise FileNotFoundError(f\"Training dataset not found at {train_data_path}\")\n",
        "if not os.path.exists(test_data_path):\n",
        "    raise FileNotFoundError(f\"Testing dataset not found at {test_data_path}\")\n",
        "\n",
        "# Load the data\n",
        "train_data_initial = pd.read_csv(train_data_path)\n",
        "test_data_initial = pd.read_csv(test_data_path)\n",
        "\n",
        "# Print the shape of the loaded data for verification\n",
        "print(f\"Training data shape: {train_data_initial.shape}\")\n",
        "print(f\"Testing data shape: {test_data_initial.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtF07XgjwRA6"
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def engineer_features(df):\n",
        "    print(\"Available columns:\", df.columns.tolist())\n",
        "\n",
        "    if 'sbytes' in df.columns and 'dbytes' in df.columns:\n",
        "        df['total_bytes'] = df['sbytes'] + df['dbytes']\n",
        "        df['byte_ratio'] = df['sbytes'] / (df['dbytes'] + 1)\n",
        "\n",
        "    if 'sttl' in df.columns and 'dttl' in df.columns:\n",
        "        df['ttl_diff'] = df['sttl'] - df['dttl']\n",
        "\n",
        "    if 'sport' in df.columns and 'dport' in df.columns:\n",
        "        df['high_port'] = ((df['sport'] > 1024) | (df['dport'] > 1024)).astype(int)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AUghSamYuwLU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available columns: ['id', 'dur', 'proto', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\n",
            "Available columns: ['id', 'dur', 'proto', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\n"
          ]
        }
      ],
      "source": [
        "# Data Preprocessing\n",
        "x_train = train_data_initial.drop(columns=['label', 'service', 'attack_cat'])\n",
        "y_train = train_data_initial['label']\n",
        "x_test = test_data_initial.drop(columns=['label', 'service', 'attack_cat'])\n",
        "y_test = test_data_initial['label']\n",
        "\n",
        "# Apply feature engineering\n",
        "x_train = engineer_features(x_train)\n",
        "x_test = engineer_features(x_test)\n",
        "\n",
        "categorical_columns = x_train.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_columns = x_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Encode categorical variables\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "x_train[categorical_columns] = encoder.fit_transform(x_train[categorical_columns])\n",
        "x_test[categorical_columns] = encoder.transform(x_test[categorical_columns])\n",
        "\n",
        "# Split the training data\n",
        "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE for oversampling\n",
        "smote = SMOTE(random_state=42)\n",
        "x_train_res, y_train_res = smote.fit_resample(x_train_split, y_train_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYxt3HdE-xpD"
      },
      "source": [
        "# Train and Evaluate the XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYze1JtU_Mxb",
        "outputId": "108fc521-f833-4fc6-b484-89baec100d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5102633241024146\n",
            "Confusion Matrix:\n",
            "[[24045 12955]\n",
            " [27366 17966]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.65      0.54     37000\n",
            "           1       0.58      0.40      0.47     45332\n",
            "\n",
            "    accuracy                           0.51     82332\n",
            "   macro avg       0.52      0.52      0.51     82332\n",
            "weighted avg       0.53      0.51      0.50     82332\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##  Initialising and training the XGBoost model  ##\n",
        "xgboost_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.003,\n",
        "    random_state=42\n",
        ")\n",
        "xgboost_model.fit(x_train_split, y_train_split)\n",
        "\n",
        "##  Check the class distribution  ##\n",
        "# print(train_data['label'].value_counts())\n",
        "# print(test_data['label'].value_counts())\n",
        "\n",
        "##  Evaluate the model's performance  ##\n",
        "y_test_pred = xgboost_model.predict(x_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
        "print(f\"Classification Report:\\n{classification_report(y_test, y_test_pred, zero_division=1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YLeKQIXhyML"
      },
      "source": [
        "# Train the Optimised XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U54kkm7h4NK",
        "outputId": "142ff41a-faf7-4b10-e0b5-cee2f85170f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized Model Accuracy: 0.7944541612009911\n",
            "Confusion Matrix:\n",
            "[[20077 16923]\n",
            " [    0 45332]]\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.54      0.70     37000\n",
            "           1       0.73      1.00      0.84     45332\n",
            "\n",
            "    accuracy                           0.79     82332\n",
            "   macro avg       0.86      0.77      0.77     82332\n",
            "weighted avg       0.85      0.79      0.78     82332\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##  Calculate scale_pos_weight as the ratio of negative to positive examples in the original data  ##\n",
        "neg, pos = y_train.value_counts()\n",
        "scale_pos_weight = neg / pos\n",
        "\n",
        "##  Define the XGBoost model  ##\n",
        "optimised_xgboost_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.003,\n",
        "    max_depth=18,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.9,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    random_state=42,\n",
        "    enable_categorical=True  # Enable built-in categorical feature support\n",
        ")\n",
        "optimised_xgboost_model.fit(x_train_res, y_train_res)\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = optimised_xgboost_model.predict(x_test)\n",
        "print(f\"Optimized Model Accuracy: {accuracy_score(y_test, test_predictions)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, test_predictions)}\")\n",
        "print(f\"Test Classification Report:\\n{classification_report(y_test, test_predictions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: c:\\Users\\kimki\\network_anomaly_detect\\system_main\\model\\saved_model\\xgboost_model.joblib\n",
            "Preprocessor saved to: c:\\Users\\kimki\\network_anomaly_detect\\system_main\\model\\saved_model\\xgboost_preprocessor.joblib\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "model_path = os.path.join(model_dir, 'xgboost_model.joblib')\n",
        "joblib.dump(optimised_xgboost_model, model_path)\n",
        "\n",
        "preprocessor = {\n",
        "    'encoder': encoder,\n",
        "    'categorical_columns': categorical_columns,\n",
        "    'numeric_columns': numeric_columns,\n",
        "    'smote': smote,\n",
        "    'scale_pos_weight': scale_pos_weight\n",
        "}\n",
        "preprocessor_path = os.path.join(model_dir, 'xgboost_preprocessor.joblib')\n",
        "joblib.dump(preprocessor, preprocessor_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Preprocessor saved to: {preprocessor_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
